{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bearing-sapphire",
   "metadata": {},
   "source": [
    "# Generate summary\n",
    "\n",
    "\n",
    "Er zijn diverse algoritmes beschikbaar voor het maken van een tekst samenvatting. Gekozen is voor een op extractie gebaseerd algoritme, omdat deze techniek gebruikt maakt van bestaande zinnen uit de tekst en dit makkelijker is dan de abstractie technieken die nieuwe zinnen maakt aan de hand van NLP-technieken. Omdat er weinig voorbeeld samenvattingen beschikbaar waren, is er gekeken naar een extractie techniek die geen voorbeelden nodig heeft om van te leren. De gekozen extractie techniek gebruikt TextRank[1], een op graven gebaseerd NLP-rangschikkingsalgoritme. Zinnen die veel connecties hebben met andere zinnen zijn waarschijnlijk ‘centrale’ zinnen en worden hoger gescoord. Ook zinnen die veel Het samenvat-algoritme is als volgt:\n",
    "\n",
    "1.\tEen selectie uit de excel wordt omgezet naar een tekstfile\n",
    "2.\tDe tekstfile wordt ingelezen en gesplitst op zinnen.\n",
    "3.\tStopwoorden en rare tekens worden verwijderd. \n",
    "4.\tElke zin wordt omgezet in een vector (word embedding).\n",
    "5.\tEen matrix van m x m zinnen wordt gecreëerd waarin de overeenkomst wordt berekend tussen de zin-vectoren met behulp van cosine similarity\n",
    "6.\tDe matrix wordt omgezet naar een graven netwerk waarin de zinnen de nodes zijn en de overeenkomst score een connectie.\n",
    "7.\tDe rangvolgorde wordt berekend aan de hand van de scores en de plek in het netwerk.\n",
    "8.\tDe N-hoogste zinnen worden geselecteerd voor de samenvatting.\n",
    "\n",
    "Nota bene: Cosine similarity gaat er van uit dat alle woorden even belangrijk zijn. Een ander veel gebruikte overeenkomst score berekening is die van TFDIDF die ervan uitgaat dat sommige woorden belangrijker zijn dan andere woorden [3]. De euclidian distance berekening is minder geschikt voor dit soort tekst vector berekeningen.\n",
    "\n",
    "Referenties\n",
    "\n",
    "[1] Mihalcea, R. and Tarau, P., 2004, July. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing (pp. 404-411).\n",
    "\n",
    "[2] Allahyari, M., Pouriyeh, S., Assefi, M., Safaei, S., Trippe, E.D., Gutierrez, J.B. and Kochut, K., 2017. Text summarization techniques: a brief survey. arXiv preprint arXiv:1707.02268.\n",
    "\n",
    "[3] Mallick, C., Das, A.K., Dutta, M., Das, A.K. and Sarkar, A., 2019. Graph-based text summarization using modified TextRank. In Soft computing in data analytics (pp. 137-146). Springer, Singapore.\n",
    "\n",
    "[4] https://github.com/edubey/text-summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "twelve-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_clean_sentences(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    text = file.read()\n",
    "    text = text.replace(r\"/\\s/g\", \"\")\n",
    "    text = re.sub(r\"(?<!\\b[A-Z])\\.(?!\\d)\", \". \", text) # this changes text.text into text. text\n",
    "    sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "    #filter out sentences that have only .\n",
    "    sent_text = [sent for sent in sent_text if len(sent) > 2]\n",
    "    sent_text = [sent.capitalize() for sent in sent_text]\n",
    "    # now loop over each sentence and tokenize it separately\n",
    "    sentences = [tokenizer.tokenize(sentence) for sentence in sent_text]\n",
    "    return sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "introductory-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "based-survivor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1][:10], sentences[idx2][:10], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('dutch')\n",
    "    summarize_text = []\n",
    "\n",
    "    print('Step 1 - Read text and split it')\n",
    "    sentences =  get_clean_sentences(file_name)\n",
    "\n",
    "    print('Step 2 - Generate Similary Matrix across sentences')\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    print('Step 3 - Rank sentences in similarity matrix')\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    print('Step 4 - Sort the rank and pick top sentences')\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)     \n",
    "    # fetch the top-n sentences (but only if they contain content)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while (i < top_n):\n",
    "        if len(ranked_sentence[j][1]) > 1:\n",
    "            summarize_text.append(\" \".join(ranked_sentence[j][1])+\".\")\n",
    "            i = i+1\n",
    "        j = j+1\n",
    "\n",
    "\n",
    "    print('Step 5 - output the summarize text')\n",
    "    print(f\"\\nSamenvatting {file_name}:\")\n",
    "    print(\"\\n\".join(summarize_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Read text and split it\n",
      "Step 2 - Generate Similary Matrix across sentences\n"
     ]
    }
   ],
   "source": [
    "# vul de lijst aan met documenten die geprocessed moeten worden\n",
    "docs = ['toon.txt']\n",
    "for file in docs:\n",
    "    generate_summary(file, 10)      \n",
    "    #try:\n",
    "    #    generate_summary(file, 10)        \n",
    "    #except:\n",
    "    #    print('deze file kan niet geprocessed worden')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biza",
   "language": "python",
   "name": "biza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
